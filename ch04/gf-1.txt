Do you think itâ€™s better for a system to have a large set of primitives or a small one, assuming that the larger set can be written in terms of the smaller one?
	- At first glance, I'm inclined to vouch for the small set of primitives. This would make it easier to learn all of the system's primitives and know them well, and higher-level abstractions can always be written in terms of these primitives.
	- On the other hand, a large set of primitives would be harder to learn, but it would also imply high-performance for this set of primitives, since they would essentially be standard for the system and therefore highly optimized (so programmers wouldn't have to write these functions themselves, a one-off will never be as good as a finely-tuned algorithm). Also, it would keep programmers from needing to write the same programs over and over again (although in the first case this would be solved by using libraries).
